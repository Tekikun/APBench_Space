{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRZC0r1SL-jK"
      },
      "outputs": [],
      "source": [
        "# Semantic Clustering Analysis for Language Model Outputs\n",
        "# Date: May 15, 2025\n",
        "# Description: This script performs a semantic analysis of language model outputs using bigram extraction,\n",
        "# spectral clustering, t-SNE visualization, and Phi coefficient-based semantic labeling.\n",
        "# It compares six models (AstroSage, AstroLlama2, Claude 3.7 Sonnet, Grok 3 Mini (high), Llama 3.1 8B, o4-mini)\n",
        "# against the PSA OpenBench Gamma dataset, as described in the accompanying paper.\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from collections import Counter\n",
        "from scipy.stats import chi2_contingency\n",
        "import warnings\n",
        "\n",
        "# Configure matplotlib to use serif font for better readability in plots\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "\n",
        "def calculate_phi_coefficient(bigrams, labels, document_blocks, n_clusters):\n",
        "    \"\"\"\n",
        "    Calculate Phi coefficients to quantify the association between bigrams and semantic categories.\n",
        "\n",
        "    Args:\n",
        "        bigrams (list): List of bigrams extracted from model outputs.\n",
        "        labels (np.ndarray): Cluster labels assigned to each bigram.\n",
        "        document_blocks (list): List of text blocks (replies) from model outputs.\n",
        "        n_clusters (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Phi scores for each cluster, mapping clusters to category scores (e.g., math, code).\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store Phi scores for each cluster and semantic category\n",
        "    phi_scores = {i: Counter() for i in range(n_clusters)}\n",
        "    doc_types = ['math', 'code', 'literary', 'dynamics']\n",
        "\n",
        "    # Classify each document block into a semantic category using regex patterns\n",
        "    doc_classifications = []\n",
        "    for block in document_blocks:\n",
        "        block_lower = block.lower()\n",
        "        if re.search(r'[0-9]+(\\.[0-9]+)?|[μΔ√πθ]', block_lower):  # Math: numbers and symbols\n",
        "            doc_classifications.append('math')\n",
        "        elif re.search(r'(def|class|if|for|while|int|str|return|[a-z_][a-z0-9_]*(\\.[a-z0-9_]+)?)', block_lower):  # Code: keywords and identifiers\n",
        "            doc_classifications.append('code')\n",
        "        elif re.search(r'(process|change|interaction|system|flow|evolve|adapt|transition|state|time)', block_lower):  # Dynamics: processes and temporal terms\n",
        "            doc_classifications.append('dynamics')\n",
        "        else:  # Literary: default category for narrative or descriptive content\n",
        "            doc_classifications.append('literary')\n",
        "\n",
        "    total_docs = len(document_blocks)\n",
        "    doc_type_counts = Counter(doc_classifications)\n",
        "\n",
        "    # Compute Phi coefficient for each bigram and semantic category\n",
        "    for i, bigram in enumerate(bigrams):\n",
        "        cluster = labels[i]\n",
        "        bigram_lower = bigram.lower()\n",
        "\n",
        "        for doc_type in doc_types:\n",
        "            if doc_type_counts[doc_type] == 0:  # Skip if no documents of this type exist\n",
        "                phi_scores[cluster][doc_type] = 0\n",
        "                continue\n",
        "\n",
        "            # Construct a 2x2 contingency table for the bigram and document type\n",
        "            a = sum(1 for block, dtype in zip(document_blocks, doc_classifications)\n",
        "                    if dtype == doc_type and bigram_lower in block.lower())  # Bigram present, doc is of type\n",
        "            b = doc_type_counts[doc_type] - a  # Bigram absent, doc is of type\n",
        "            c = sum(1 for block, dtype in zip(document_blocks, doc_classifications)\n",
        "                    if dtype != doc_type and bigram_lower in block.lower())  # Bigram present, doc not of type\n",
        "            d = (total_docs - doc_type_counts[doc_type]) - c  # Bigram absent, doc not of type\n",
        "\n",
        "            # Apply Laplace smoothing (alpha=0.5) to avoid division by zero\n",
        "            smoothing = 0.5\n",
        "            table = np.array([[a + smoothing, b + smoothing],\n",
        "                              [c + smoothing, d + smoothing]])\n",
        "\n",
        "            # Compute chi-squared statistic and Phi coefficient\n",
        "            chi2, _, _, _ = chi2_contingency(table, correction=False)\n",
        "            phi = np.sqrt(chi2 / (total_docs + 4 * smoothing)) if total_docs > 0 else 0\n",
        "            phi_scores[cluster][doc_type] += phi\n",
        "\n",
        "    return phi_scores\n",
        "\n",
        "def plot_clusters(bigrams, labels, n_clusters, name, ax, semantic_labels, axis_limits):\n",
        "    \"\"\"\n",
        "    Plot t-SNE clusters for a model's bigrams with semantic labels.\n",
        "\n",
        "    Args:\n",
        "        bigrams (list): List of bigrams to plot.\n",
        "        labels (np.ndarray): Cluster labels for the bigrams.\n",
        "        n_clusters (int): Number of clusters.\n",
        "        name (str): Model name for the plot title.\n",
        "        ax (matplotlib.axes.Axes): Axis object to plot on.\n",
        "        semantic_labels (dict): Mapping of cluster IDs to semantic labels.\n",
        "        axis_limits (tuple): Tuple of (x_limits, y_limits) for uniform axis scaling.\n",
        "    \"\"\"\n",
        "    # Handle empty or insufficient data\n",
        "    if not bigrams:\n",
        "        ax.text(0.5, 0.5, 'Data Unavailable', ha='center', va='center', fontsize=12, color='red')\n",
        "        ax.set_title(name)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        return\n",
        "\n",
        "    # Vectorize bigrams into a sparse matrix of token counts\n",
        "    vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "    X = vectorizer.fit_transform(bigrams).toarray()\n",
        "\n",
        "    # Apply t-SNE for 2D visualization, dynamically tuning perplexity\n",
        "    perplexity = min(30, max(5, len(bigrams)-1))\n",
        "    tsne_coords = TSNE(n_components=2, perplexity=perplexity, random_state=42).fit_transform(X)\n",
        "\n",
        "    # Define cluster labels and colors for the 8-cluster case\n",
        "    label_order = ['Math Basics', 'Math Advanced', 'Code Syntax', 'Code Structure',\n",
        "                   'Literary Prose', 'Literary Themes', 'Dynamics Basics', 'Dynamics Advanced']\n",
        "    colors = ['#1f77b4', '#4a90e2', '#ff7f0e', '#ffaa5e', '#2ca02c', '#6abe6a', '#d62728', '#ff6666']\n",
        "\n",
        "    # Map clusters to labels and sort by predefined order\n",
        "    cluster_mapping = []\n",
        "    for i in range(n_clusters):\n",
        "        cluster_label = semantic_labels.get(f'Cluster {i+1}', f'Cluster {i+1}')\n",
        "        if cluster_label in label_order:\n",
        "            cluster_mapping.append((i, cluster_label, label_order.index(cluster_label)))\n",
        "\n",
        "    cluster_mapping.sort(key=lambda x: x[2])\n",
        "\n",
        "    # Remove existing legend if present\n",
        "    if ax.get_legend():\n",
        "        ax.get_legend().remove()\n",
        "\n",
        "    # Plot each cluster with its corresponding color and label\n",
        "    handles = []\n",
        "    for i, cluster_label, color_idx in cluster_mapping:\n",
        "        idx = np.where(labels == i)[0]\n",
        "        scatter = ax.scatter(tsne_coords[idx, 0], tsne_coords[idx, 1], c=[colors[color_idx]],\n",
        "                             alpha=0.8, s=20)\n",
        "        handles.append(scatter)\n",
        "\n",
        "    # Add legend with cluster labels\n",
        "    ax.legend(handles, [label for _, label, _ in cluster_mapping], loc='best')\n",
        "\n",
        "    # Set plot title and axis labels\n",
        "    ax.set_title(name)\n",
        "    ax.set_xlabel(r't-SNE $p_1$')\n",
        "    ax.set_ylabel(r't-SNE $p_2$')\n",
        "\n",
        "    # Apply uniform axis limits if provided\n",
        "    if axis_limits:\n",
        "        ax.set_xlim(axis_limits[0])\n",
        "        ax.set_ylim(axis_limits[1])\n",
        "\n",
        "def assign_semantic_labels(phi_scores, n_clusters):\n",
        "    \"\"\"\n",
        "    Assign semantic labels to clusters based on Phi coefficient scores.\n",
        "\n",
        "    Args:\n",
        "        phi_scores (dict): Phi scores mapping clusters to semantic category scores.\n",
        "        n_clusters (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping of cluster IDs (e.g., 'Cluster 1') to semantic labels.\n",
        "    \"\"\"\n",
        "    labels = {}\n",
        "    if n_clusters == 8:\n",
        "        # Collect scores for each semantic category across clusters\n",
        "        math_scores = []\n",
        "        code_scores = []\n",
        "        literary_scores = []\n",
        "        dynamics_scores = []\n",
        "\n",
        "        for cluster in range(n_clusters):\n",
        "            scores = phi_scores[cluster]\n",
        "            math_scores.append(scores['math'])\n",
        "            code_scores.append(scores['code'])\n",
        "            literary_scores.append(scores['literary'])\n",
        "            dynamics_scores.append(scores['dynamics'])\n",
        "\n",
        "        # Sort clusters by their association with each category\n",
        "        math_pairs = sorted([(score, idx) for idx, score in enumerate(math_scores)], reverse=True)\n",
        "        code_pairs = sorted([(score, idx) for idx, score in enumerate(code_scores)], reverse=True)\n",
        "        literary_pairs = sorted([(score, idx) for idx, score in enumerate(literary_scores)], reverse=True)\n",
        "        dynamics_pairs = sorted([(score, idx) for idx, score in enumerate(dynamics_scores)], reverse=True)\n",
        "\n",
        "        # Assign labels to clusters, ensuring no cluster is assigned multiple labels\n",
        "        used_clusters = set()\n",
        "        assigned_labels = [\n",
        "            ('Math Basics', math_pairs[0][1]), ('Math Advanced', math_pairs[1][1]),\n",
        "            ('Code Syntax', code_pairs[0][1]), ('Code Structure', code_pairs[1][1]),\n",
        "            ('Literary Prose', literary_pairs[0][1]), ('Literary Themes', literary_pairs[1][1]),\n",
        "            ('Dynamics Basics', dynamics_pairs[0][1]), ('Dynamics Advanced', dynamics_pairs[1][1])\n",
        "        ]\n",
        "\n",
        "        for label, cluster_idx in assigned_labels:\n",
        "            if cluster_idx not in used_clusters:\n",
        "                labels[f'Cluster {cluster_idx+1}'] = label\n",
        "                used_clusters.add(cluster_idx)\n",
        "            else:\n",
        "                # If a cluster is already used, find the next highest-scoring unused cluster\n",
        "                for _, next_cluster in math_pairs + code_pairs + literary_pairs + dynamics_pairs:\n",
        "                    if next_cluster not in used_clusters:\n",
        "                        labels[f'Cluster {next_cluster+1}'] = label\n",
        "                        used_clusters.add(next_cluster)\n",
        "                        break\n",
        "    return labels\n",
        "\n",
        "def get_axis_limits(all_tsne_coords):\n",
        "    \"\"\"\n",
        "    Compute uniform axis limits for t-SNE plots with padding.\n",
        "\n",
        "    Args:\n",
        "        all_tsne_coords (list): List of t-SNE coordinates for all models.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple of (x_limits, y_limits) for uniform scaling.\n",
        "    \"\"\"\n",
        "    x_min, x_max = np.inf, -np.inf\n",
        "    y_min, y_max = np.inf, -np.inf\n",
        "    for coords in all_tsne_coords:\n",
        "        if coords.size > 0:\n",
        "            x_min = min(x_min, coords[:, 0].min())\n",
        "            x_max = max(x_max, coords[:, 0].max())\n",
        "            y_min = min(y_min, coords[:, 1].min())\n",
        "            y_max = max(y_max, coords[:, 1].max())\n",
        "    x_pad = 0.1 * (x_max - x_min)  # 10% padding on x-axis\n",
        "    y_pad = 0.1 * (y_max - y_min)  # 10% padding on y-axis\n",
        "    return (x_min - x_pad, x_max + x_pad), (y_min - y_pad, y_max + y_pad)\n",
        "\n",
        "# Main execution logic for clustering and visualization\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the list of models to compare\n",
        "    models = ['AstroSage', 'AstroLlama2', 'Claude 3.7 Sonnet', 'Grok 3 Mini (high)', 'Llama 3.1 8B', 'o4-mini']\n",
        "    n_clusters = 8  # Number of clusters for spectral clustering\n",
        "\n",
        "    # Set up a 2x3 subplot grid for the six models\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # First pass: Compute t-SNE coordinates to determine uniform axis limits\n",
        "    all_tsne_coords = []\n",
        "    for i, name in enumerate(models):\n",
        "        unique_bigrams = []\n",
        "        document_blocks = []\n",
        "\n",
        "        # Load and parse model output file\n",
        "        filename = f\"{name}.txt\"\n",
        "        try:\n",
        "            with open(filename, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                # Extract reply blocks using regex patterns\n",
        "                document_blocks = re.findall(r'reply\\s*:\\s*(.*?)(?=\\nuser_answer_str:|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    document_blocks = re.findall(r'The answer should either be numeric or symbolic \\(not words\\)\\s*.*?\\n(.*?)(?=\\nMESSAGE|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    print(f\"Warning: No reply blocks found in {filename}.\")\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "                    continue\n",
        "\n",
        "                # Tokenize and generate bigrams\n",
        "                text = \"\\n\".join(document_blocks)\n",
        "                tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "                if not tokens:\n",
        "                    print(f\"Warning: No tokens extracted from {filename}.\")\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "                    continue\n",
        "\n",
        "                bigrams = [' '.join(tokens[j:j+2]) for j in range(len(tokens)-1)]\n",
        "                unique_bigrams = list(Counter(bigrams).keys())\n",
        "                if not unique_bigrams:\n",
        "                    print(f\"Warning: No unique bigrams generated from {filename}.\")\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "                    continue\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: {filename} not found.\")\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "            continue\n",
        "\n",
        "        # Ensure enough bigrams for clustering\n",
        "        if len(unique_bigrams) < n_clusters:\n",
        "            print(f\"Warning: Not enough unique bigrams ({len(unique_bigrams)}) in {name} to form {n_clusters} clusters.\")\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "            continue\n",
        "\n",
        "        # Vectorize bigrams and apply spectral clustering\n",
        "        vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "        X = vectorizer.fit_transform(unique_bigrams).toarray()\n",
        "        labels = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans', random_state=42).fit_predict(X)\n",
        "\n",
        "        # Calculate Phi coefficients and assign semantic labels\n",
        "        phi_scores = calculate_phi_coefficient(unique_bigrams, labels, document_blocks, n_clusters)\n",
        "        semantic_labels = assign_semantic_labels(phi_scores, n_clusters)\n",
        "\n",
        "        # Compute t-SNE coordinates for axis limit calculation\n",
        "        perplexity = min(30, max(5, len(unique_bigrams)-1))\n",
        "        tsne_coords = TSNE(n_components=2, perplexity=perplexity, random_state=42).fit_transform(X)\n",
        "        all_tsne_coords.append(tsne_coords)\n",
        "\n",
        "        # Print semantic labels for verification\n",
        "        print(f\"\\n{name} ({n_clusters} clusters) Semantic Labels:\")\n",
        "        for cluster, meaning in semantic_labels.items():\n",
        "            print(f\"{cluster}: {meaning}\")\n",
        "\n",
        "    # Compute uniform axis limits across all models\n",
        "    axis_limits = get_axis_limits(all_tsne_coords) if all_tsne_coords else None\n",
        "\n",
        "    # Second pass: Plot the clusters with uniform axis limits\n",
        "    for i, name in enumerate(models):\n",
        "        unique_bigrams = []\n",
        "        document_blocks = []\n",
        "\n",
        "        filename = f\"{name}.txt\"\n",
        "        try:\n",
        "            with open(filename, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                document_blocks = re.findall(r'reply\\s*:\\s*(.*?)(?=\\nuser_answer_str:|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    document_blocks = re.findall(r'The answer should either be numeric or symbolic \\(not words\\)\\s*.*?\\n(.*?)(?=\\nMESSAGE|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "                    continue\n",
        "                text = \"\\n\".join(document_blocks)\n",
        "                tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "                if not tokens:\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "                    continue\n",
        "                bigrams = [' '.join(tokens[j:j+2]) for j in range(len(tokens)-1)]\n",
        "                unique_bigrams = list(Counter(bigrams).keys())\n",
        "                if not unique_bigrams:\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "                    continue\n",
        "        except FileNotFoundError:\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "            continue\n",
        "\n",
        "        if len(unique_bigrams) < n_clusters:\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "            continue\n",
        "\n",
        "        # Vectorize, cluster, and label for plotting\n",
        "        vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "        X = vectorizer.fit_transform(unique_bigrams).toarray()\n",
        "        labels = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans', random_state=42).fit_predict(X)\n",
        "        phi_scores = calculate_phi_coefficient(unique_bigrams, labels, document_blocks, n_clusters)\n",
        "        semantic_labels = assign_semantic_labels(phi_scores, n_clusters)\n",
        "\n",
        "        # Plot the t-SNE clusters\n",
        "        plot_clusters(unique_bigrams, labels, n_clusters, name, axes[i], semantic_labels, axis_limits)\n",
        "\n",
        "    # Add a super title and adjust layout for the final plot\n",
        "    plt.suptitle(f\"t-SNE Clustering Comparison ({n_clusters} Clusters)\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    # Save the plot as an SVG file for high-quality rendering in the paper\n",
        "    plt.savefig(f\"comparison_models_{n_clusters}_clusters.svg\", bbox_inches='tight', dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Clustering Analysis for Language Model Outputs (4 Clusters)\n",
        "# Date: May 15, 2025\n",
        "# Description: This script performs a semantic analysis of language model outputs using bigram extraction,\n",
        "# spectral clustering, t-SNE visualization, and Phi coefficient-based semantic labeling for 4 clusters.\n",
        "# It compares six models (AstroSage, AstroLlama2, Claude 3.7 Sonnet, Grok 3 Mini (high), Llama 3.1 8B, o4-mini)\n",
        "# against the PSA OpenBench Gamma dataset, as described in the accompanying paper.\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from collections import Counter\n",
        "from scipy.stats import chi2_contingency\n",
        "import warnings\n",
        "\n",
        "# Configure matplotlib to use serif font for better readability in plots\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "\n",
        "def calculate_phi_coefficient(bigrams, labels, document_blocks, n_clusters):\n",
        "    \"\"\"\n",
        "    Calculate Phi coefficients to quantify the association between bigrams and semantic categories.\n",
        "\n",
        "    Args:\n",
        "        bigrams (list): List of bigrams extracted from model outputs.\n",
        "        labels (np.ndarray): Cluster labels assigned to each bigram.\n",
        "        document_blocks (list): List of text blocks (replies) from model outputs.\n",
        "        n_clusters (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Phi scores for each cluster, mapping clusters to category scores (e.g., math, code).\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store Phi scores for each cluster and semantic category\n",
        "    phi_scores = {i: Counter() for i in range(n_clusters)}\n",
        "    doc_types = ['math', 'code', 'literary', 'dynamics']\n",
        "\n",
        "    # Classify each document block into a semantic category using regex patterns\n",
        "    doc_classifications = []\n",
        "    for block in document_blocks:\n",
        "        block_lower = block.lower()\n",
        "        if re.search(r'[0-9]+(\\.[0-9]+)?|[μΔ√πθ]', block_lower):  # Math: numbers and symbols\n",
        "            doc_classifications.append('math')\n",
        "        elif re.search(r'(def|class|if|for|while|int|str|return|[a-z_][a-z0-9_]*(\\.[a-z0-9_]+)?)', block_lower):  # Code: keywords and identifiers\n",
        "            doc_classifications.append('code')\n",
        "        elif re.search(r'(process|change|interaction|system|flow|evolve|adapt|transition|state|time)', block_lower):  # Dynamics: processes and temporal terms\n",
        "            doc_classifications.append('dynamics')\n",
        "        else:  # Literary: narrative or descriptive content\n",
        "            doc_classifications.append('literary')\n",
        "\n",
        "    total_docs = len(document_blocks)\n",
        "    doc_type_counts = Counter(doc_classifications)\n",
        "\n",
        "    # Compute Phi coefficient for each bigram and semantic category\n",
        "    for i, bigram in enumerate(bigrams):\n",
        "        cluster = labels[i]\n",
        "        bigram_lower = bigram.lower()\n",
        "\n",
        "        for doc_type in doc_types:\n",
        "            if doc_type_counts[doc_type] == 0:  # Skip if no documents of this type exist\n",
        "                phi_scores[cluster][doc_type] = 0\n",
        "                continue\n",
        "\n",
        "            # Construct a 2x2 contingency table for the bigram and document type\n",
        "            a = sum(1 for block, dtype in zip(document_blocks, doc_classifications)\n",
        "                    if dtype == doc_type and bigram_lower in block.lower())  # Bigram present, doc is of type\n",
        "            b = doc_type_counts[doc_type] - a  # Bigram absent, doc is of type\n",
        "            c = sum(1 for block, dtype in zip(document_blocks, doc_classifications)\n",
        "                    if dtype != doc_type and bigram_lower in block.lower())  # Bigram present, doc not of type\n",
        "            d = (total_docs - doc_type_counts[doc_type]) - c  # Bigram absent, doc not of type\n",
        "\n",
        "            # Apply Laplace smoothing (alpha=0.5) to avoid division by zero\n",
        "            smoothing = 0.5\n",
        "            table = np.array([[a + smoothing, b + smoothing],\n",
        "                              [c + smoothing, d + smoothing]])\n",
        "\n",
        "            # Compute chi-squared statistic and Phi coefficient\n",
        "            chi2, _, _, _ = chi2_contingency(table, correction=False)\n",
        "            phi = np.sqrt(chi2 / (total_docs + 4 * smoothing)) if total_docs > 0 else 0\n",
        "            phi_scores[cluster][doc_type] += phi\n",
        "\n",
        "    return phi_scores\n",
        "\n",
        "def plot_clusters(bigrams, labels, n_clusters, name, ax, semantic_labels, axis_limits):\n",
        "    \"\"\"\n",
        "    Plot t-SNE clusters for a model's bigrams with semantic labels.\n",
        "\n",
        "    Args:\n",
        "        bigrams (list): List of bigrams to plot.\n",
        "        labels (np.ndarray): Cluster labels for the bigrams.\n",
        "        n_clusters (int): Number of clusters.\n",
        "        name (str): Model name for the plot title.\n",
        "        ax (matplotlib.axes.Axes): Axis object to plot on.\n",
        "        semantic_labels (dict): Mapping of cluster IDs to semantic labels.\n",
        "        axis_limits (tuple): Tuple of (x_limits, y_limits) for uniform axis scaling.\n",
        "    \"\"\"\n",
        "    # Handle empty or insufficient data\n",
        "    if not bigrams:\n",
        "        ax.text(0.5, 0.5, 'Data Unavailable', ha='center', va='center', fontsize=12, color='red')\n",
        "        ax.set_title(name)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        return\n",
        "\n",
        "    # Vectorize bigrams into a sparse matrix of token counts\n",
        "    vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "    X = vectorizer.fit_transform(bigrams).toarray()\n",
        "\n",
        "    # Apply t-SNE for 2D visualization, dynamically tuning perplexity\n",
        "    perplexity = min(30, max(5, len(bigrams)-1))\n",
        "    tsne_coords = TSNE(n_components=2, perplexity=perplexity, random_state=42).fit_transform(X)\n",
        "\n",
        "    # Define cluster labels and colors for the 4-cluster case\n",
        "    label_order = ['Math', 'Code', 'Literary', 'Dynamics']\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "\n",
        "    # Map clusters to labels and sort by predefined order\n",
        "    cluster_mapping = []\n",
        "    for i in range(n_clusters):\n",
        "        cluster_label = semantic_labels.get(f'Cluster {i+1}', f'Cluster {i+1}')\n",
        "        if cluster_label in label_order:\n",
        "            cluster_mapping.append((i, cluster_label, label_order.index(cluster_label)))\n",
        "\n",
        "    cluster_mapping.sort(key=lambda x: x[2])\n",
        "\n",
        "    # Remove existing legend if present\n",
        "    if ax.get_legend():\n",
        "        ax.get_legend().remove()\n",
        "\n",
        "    # Plot each cluster with its corresponding color and label\n",
        "    handles = []\n",
        "    for i, cluster_label, color_idx in cluster_mapping:\n",
        "        idx = np.where(labels == i)[0]\n",
        "        scatter = ax.scatter(tsne_coords[idx, 0], tsne_coords[idx, 1], c=[colors[color_idx]],\n",
        "                             alpha=0.8, s=20)\n",
        "        handles.append(scatter)\n",
        "\n",
        "    # Add legend with cluster labels\n",
        "    ax.legend(handles, [label for _, label, _ in cluster_mapping], loc='best')\n",
        "\n",
        "    # Set plot title and axis labels\n",
        "    ax.set_title(name)\n",
        "    ax.set_xlabel(r't-SNE $p_1$')\n",
        "    ax.set_ylabel(r't-SNE $p_2$')\n",
        "\n",
        "    # Apply uniform axis limits if provided\n",
        "    if axis_limits:\n",
        "        ax.set_xlim(axis_limits[0])\n",
        "        ax.set_ylim(axis_limits[1])\n",
        "\n",
        "def assign_semantic_labels(phi_scores, n_clusters):\n",
        "    \"\"\"\n",
        "    Assign semantic labels to clusters based on Phi coefficient scores for 4-cluster analysis.\n",
        "\n",
        "    Args:\n",
        "        phi_scores (dict): Phi scores mapping clusters to semantic category scores.\n",
        "        n_clusters (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping of cluster IDs (e.g., 'Cluster 1') to semantic labels.\n",
        "    \"\"\"\n",
        "    labels = {}\n",
        "    if n_clusters == 4:\n",
        "        # Determine the dominant semantic category for each cluster\n",
        "        sorted_scores = {k: max(v.items(), key=lambda x: x[1])[0] for k, v in phi_scores.items()}\n",
        "        assigned_types = ['Math', 'Code', 'Literary', 'Dynamics']\n",
        "        used_types = set()\n",
        "        for cluster in range(n_clusters):\n",
        "            doc_type = sorted_scores[cluster]\n",
        "            # Assign labels while ensuring uniqueness\n",
        "            if doc_type == 'math' and 'Math' not in used_types:\n",
        "                labels[f'Cluster {cluster+1}'] = 'Math'\n",
        "                used_types.add('Math')\n",
        "            elif doc_type == 'code' and 'Code' not in used_types:\n",
        "                labels[f'Cluster {cluster+1}'] = 'Code'\n",
        "                used_types.add('Code')\n",
        "            elif doc_type == 'literary' and 'Literary' not in used_types:\n",
        "                labels[f'Cluster {cluster+1}'] = 'Literary'\n",
        "                used_types.add('Literary')\n",
        "            elif doc_type == 'dynamics' and 'Dynamics' not in used_types:\n",
        "                labels[f'Cluster {cluster+1}'] = 'Dynamics'\n",
        "                used_types.add('Dynamics')\n",
        "            else:\n",
        "                # If a category is already used, assign the next available label\n",
        "                for t in assigned_types:\n",
        "                    if t not in used_types:\n",
        "                        labels[f'Cluster {cluster+1}'] = t\n",
        "                        used_types.add(t)\n",
        "                        break\n",
        "    return labels\n",
        "\n",
        "def get_axis_limits(all_tsne_coords):\n",
        "    \"\"\"\n",
        "    Compute uniform axis limits for t-SNE plots with padding.\n",
        "\n",
        "    Args:\n",
        "        all_tsne_coords (list): List of t-SNE coordinates for all models.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple of (x_limits, y_limits) for uniform scaling.\n",
        "    \"\"\"\n",
        "    x_min, x_max = np.inf, -np.inf\n",
        "    y_min, y_max = np.inf, -np.inf\n",
        "    for coords in all_tsne_coords:\n",
        "        if coords.size > 0:\n",
        "            x_min = min(x_min, coords[:, 0].min())\n",
        "            x_max = max(x_max, coords[:, 0].max())\n",
        "            y_min = min(y_min, coords[:, 1].min())\n",
        "            y_max = max(y_max, coords[:, 1].max())\n",
        "    x_pad = 0.1 * (x_max - x_min)  # 10% padding on x-axis\n",
        "    y_pad = 0.1 * (y_max - y_min)  # 10% padding on y-axis\n",
        "    return (x_min - x_pad, x_max + x_pad), (y_min - y_pad, y_max + y_pad)\n",
        "\n",
        "# Main execution logic for clustering and visualization\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the list of models to compare\n",
        "    models = ['AstroSage', 'AstroLlama2', 'Claude 3.7 Sonnet', 'Grok 3 Mini (high)', 'Llama 3.1 8B', 'o4-mini']\n",
        "    n_clusters = 4  # Number of clusters for spectral clustering\n",
        "\n",
        "    # Set up a 2x3 subplot grid for the six models\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # First pass: Compute t-SNE coordinates to determine uniform axis limits\n",
        "    all_tsne_coords = []\n",
        "    for i, name in enumerate(models):\n",
        "        unique_bigrams = []\n",
        "        document_blocks = []\n",
        "\n",
        "        # Load and parse model output file\n",
        "        filename = f\"{name}.txt\"\n",
        "        try:\n",
        "            with open(filename, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                # Extract reply blocks using regex patterns\n",
        "                document_blocks = re.findall(r'reply\\s*:\\s*(.*?)(?=\\nuser_answer_str:|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    document_blocks = re.findall(r'The answer should either be numeric or symbolic \\(not words\\)\\s*.*?\\n(.*?)(?=\\nMESSAGE|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    print(f\"Warning: No reply blocks found in {filename}.\")\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "                    continue\n",
        "\n",
        "                # Tokenize and generate bigrams\n",
        "                text = \"\\n\".join(document_blocks)\n",
        "                tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "                if not tokens:\n",
        "                    print(f\"Warning: No tokens extracted from {filename}.\")\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "                    continue\n",
        "\n",
        "                bigrams = [' '.join(tokens[j:j+2]) for j in range(len(tokens)-1)]\n",
        "                unique_bigrams = list(Counter(bigrams).keys())\n",
        "                if not unique_bigrams:\n",
        "                    print(f\"Warning: No unique bigrams generated from {filename}.\")\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "                    continue\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: {filename} not found.\")\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "            continue\n",
        "\n",
        "        # Ensure enough bigrams for clustering\n",
        "        if len(unique_bigrams) < n_clusters:\n",
        "            print(f\"Warning: Not enough unique bigrams ({len(unique_bigrams)}) in {name} to form {n_clusters} clusters.\")\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "            continue\n",
        "\n",
        "        # Vectorize bigrams and apply spectral clustering\n",
        "        vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "        X = vectorizer.fit_transform(unique_bigrams).toarray()\n",
        "        labels = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans', random_state=42).fit_predict(X)\n",
        "\n",
        "        # Calculate Phi coefficients and assign semantic labels\n",
        "        phi_scores = calculate_phi_coefficient(unique_bigrams, labels, document_blocks, n_clusters)\n",
        "        semantic_labels = assign_semantic_labels(phi_scores, n_clusters)\n",
        "\n",
        "        # Compute t-SNE coordinates for axis limit calculation\n",
        "        perplexity = min(30, max(5, len(unique_bigrams)-1))\n",
        "        tsne_coords = TSNE(n_components=2, perplexity=perplexity, random_state=42).fit_transform(X)\n",
        "        all_tsne_coords.append(tsne_coords)\n",
        "\n",
        "        # Print semantic labels for verification\n",
        "        print(f\"\\n{name} ({n_clusters} clusters) Semantic Labels:\")\n",
        "        for cluster, meaning in semantic_labels.items():\n",
        "            print(f\"{cluster}: {meaning}\")\n",
        "\n",
        "    # Compute uniform axis limits across all models\n",
        "    axis_limits = get_axis_limits(all_tsne_coords) if all_tsne_coords else None\n",
        "\n",
        "    # Second pass: Plot the clusters with uniform axis limits\n",
        "    for i, name in enumerate(models):\n",
        "        unique_bigrams = []\n",
        "        document_blocks = []\n",
        "\n",
        "        filename = f\"{name}.txt\"\n",
        "        try:\n",
        "            with open(filename, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                document_blocks = re.findall(r'reply\\s*:\\s*(.*?)(?=\\nuser_answer_str:|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    document_blocks = re.findall(r'The answer should either be numeric or symbolic \\(not words\\)\\s*.*?\\n(.*?)(?=\\nMESSAGE|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "                    continue\n",
        "                text = \"\\n\".join(document_blocks)\n",
        "                tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "                if not tokens:\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "                    continue\n",
        "                bigrams = [' '.join(tokens[j:j+2]) for j in range(len(tokens)-1)]\n",
        "                unique_bigrams = list(Counter(bigrams).keys())\n",
        "                if not unique_bigrams:\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "                    continue\n",
        "        except FileNotFoundError:\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "            continue\n",
        "\n",
        "        if len(unique_bigrams) < n_clusters:\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "            continue\n",
        "\n",
        "        # Vectorize, cluster, and label for plotting\n",
        "        vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "        X = vectorizer.fit_transform(unique_bigrams).toarray()\n",
        "        labels = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans', random_state=42).fit_predict(X)\n",
        "        phi_scores = calculate_phi_coefficient(unique_bigrams, labels, document_blocks, n_clusters)\n",
        "        semantic_labels = assign_semantic_labels(phi_scores, n_clusters)\n",
        "\n",
        "        # Plot the t-SNE clusters\n",
        "        plot_clusters(unique_bigrams, labels, n_clusters, name, axes[i], semantic_labels, axis_limits)\n",
        "\n",
        "    # Add a super title and adjust layout for the final plot\n",
        "    plt.suptitle(f\"t-SNE Clustering Comparison ({n_clusters} Clusters)\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    # Save the plot as an SVG file for high-quality rendering in the paper\n",
        "    plt.savefig(f\"comparison_models_{n_clusters}_clusters.svg\", bbox_inches='tight', dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Aacllo1cRfH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic Clustering Analysis for Language Model Outputs (2 Clusters)\n",
        "# Date: May 15, 2025\n",
        "# Description: This script performs a semantic analysis of language model outputs using bigram extraction,\n",
        "# spectral clustering, t-SNE visualization, and Phi coefficient-based semantic labeling for 2 clusters.\n",
        "# It compares six models (AstroSage, AstroLlama2, Claude 3.7 Sonnet, Grok 3 Mini (high), Llama 3.1 8B, o4-mini)\n",
        "# against the PSA OpenBench Gamma dataset, as described in the accompanying paper.\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from collections import Counter\n",
        "from scipy.stats import chi2_contingency\n",
        "import warnings\n",
        "\n",
        "# Configure matplotlib to use serif font for better readability in plots\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "\n",
        "def calculate_phi_coefficient(bigrams, labels, document_blocks, n_clusters):\n",
        "    \"\"\"\n",
        "    Calculate Phi coefficients to quantify the association between bigrams and semantic categories.\n",
        "\n",
        "    Args:\n",
        "        bigrams (list): List of bigrams extracted from model outputs.\n",
        "        labels (np.ndarray): Cluster labels assigned to each bigram.\n",
        "        document_blocks (list): List of text blocks (replies) from model outputs.\n",
        "        n_clusters (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Phi scores for each cluster, mapping clusters to category scores (e.g., math, code).\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store Phi scores for each cluster and semantic category\n",
        "    phi_scores = {i: Counter() for i in range(n_clusters)}\n",
        "    doc_types = ['math', 'code', 'literary', 'dynamics']\n",
        "\n",
        "    # Classify each document block into a semantic category using regex patterns\n",
        "    doc_classifications = []\n",
        "    for block in document_blocks:\n",
        "        block_lower = block.lower()\n",
        "        if re.search(r'[0-9]+(\\.[0-9]+)?|[μΔ√πθ]', block_lower):  # Math: numbers and symbols\n",
        "            doc_classifications.append('math')\n",
        "        elif re.search(r'(def|class|if|for|while|int|str|return|[a-z_][a-z0-9_]*(\\.[a-z0-9_]+)?)', block_lower):  # Code: keywords and identifiers\n",
        "            doc_classifications.append('code')\n",
        "        elif re.search(r'(process|change|interaction|system|flow|evolve|adapt|transition|state|time)', block_lower):  # Dynamics: processes and temporal terms\n",
        "            doc_classifications.append('dynamics')\n",
        "        else:  # Literary: narrative or descriptive content\n",
        "            doc_classifications.append('literary')\n",
        "\n",
        "    total_docs = len(document_blocks)\n",
        "    doc_type_counts = Counter(doc_classifications)\n",
        "\n",
        "    # Compute Phi coefficient for each bigram and semantic category\n",
        "    for i, bigram in enumerate(bigrams):\n",
        "        cluster = labels[i]\n",
        "        bigram_lower = bigram.lower()\n",
        "\n",
        "        for doc_type in doc_types:\n",
        "            if doc_type_counts[doc_type] == 0:  # Skip if no documents of this type exist\n",
        "                phi_scores[cluster][doc_type] = 0\n",
        "                continue\n",
        "\n",
        "            # Construct a 2x2 contingency table for the bigram and document type\n",
        "            a = sum(1 for block, dtype in zip(document_blocks, doc_classifications)\n",
        "                    if dtype == doc_type and bigram_lower in block.lower())  # Bigram present, doc is of type\n",
        "            b = doc_type_counts[doc_type] - a  # Bigram absent, doc is of type\n",
        "            c = sum(1 for block, dtype in zip(document_blocks, doc_classifications)\n",
        "                    if dtype != doc_type and bigram_lower in block.lower())  # Bigram present, doc not of type\n",
        "            d = (total_docs - doc_type_counts[doc_type]) - c  # Bigram absent, doc not of type\n",
        "\n",
        "            # Apply Laplace smoothing (alpha=0.5) to avoid division by zero\n",
        "            smoothing = 0.5\n",
        "            table = np.array([[a + smoothing, b + smoothing],\n",
        "                              [c + smoothing, d + smoothing]])\n",
        "\n",
        "            # Compute chi-squared statistic and Phi coefficient\n",
        "            chi2, _, _, _ = chi2_contingency(table, correction=False)\n",
        "            phi = np.sqrt(chi2 / (total_docs + 4 * smoothing)) if total_docs > 0 else 0\n",
        "            phi_scores[cluster][doc_type] += phi\n",
        "\n",
        "    return phi_scores\n",
        "\n",
        "def plot_clusters(bigrams, labels, n_clusters, name, ax, semantic_labels, axis_limits):\n",
        "    \"\"\"\n",
        "    Plot t-SNE clusters for a model's bigrams with semantic labels.\n",
        "\n",
        "    Args:\n",
        "        bigrams (list): List of bigrams to plot.\n",
        "        labels (np.ndarray): Cluster labels for the bigrams.\n",
        "        n_clusters (int): Number of clusters.\n",
        "        name (str): Model name for the plot title.\n",
        "        ax (matplotlib.axes.Axes): Axis object to plot on.\n",
        "        semantic_labels (dict): Mapping of cluster IDs to semantic labels.\n",
        "        axis_limits (tuple): Tuple of (x_limits, y_limits) for uniform axis scaling.\n",
        "    \"\"\"\n",
        "    # Handle empty or insufficient data\n",
        "    if not bigrams:\n",
        "        ax.text(0.5, 0.5, 'Data Unavailable', ha='center', va='center', fontsize=12, color='red')\n",
        "        ax.set_title(name)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        return\n",
        "\n",
        "    # Vectorize bigrams into a sparse matrix of token counts\n",
        "    vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "    X = vectorizer.fit_transform(bigrams).toarray()\n",
        "\n",
        "    # Apply t-SNE for 2D visualization, dynamically tuning perplexity\n",
        "    perplexity = min(30, max(5, len(bigrams)-1))\n",
        "    tsne_coords = TSNE(n_components=2, perplexity=perplexity, random_state=42).fit_transform(X)\n",
        "\n",
        "    # Define cluster labels and colors for the 2-cluster case (Technical, Narrative)\n",
        "    label_order = ['Technical', 'Narrative']\n",
        "    colors = ['#1f77b4', '#ff7f0e']\n",
        "\n",
        "    # Map clusters to labels and sort by predefined order\n",
        "    cluster_mapping = []\n",
        "    for i in range(n_clusters):\n",
        "        cluster_label = semantic_labels.get(f'Cluster {i+1}', f'Cluster {i+1}')\n",
        "        if cluster_label in label_order:\n",
        "            cluster_mapping.append((i, cluster_label, label_order.index(cluster_label)))\n",
        "\n",
        "    cluster_mapping.sort(key=lambda x: x[2])\n",
        "\n",
        "    # Remove existing legend if present\n",
        "    if ax.get_legend():\n",
        "        ax.get_legend().remove()\n",
        "\n",
        "    # Plot each cluster with its corresponding color and label\n",
        "    handles = []\n",
        "    for i, cluster_label, color_idx in cluster_mapping:\n",
        "        idx = np.where(labels == i)[0]\n",
        "        scatter = ax.scatter(tsne_coords[idx, 0], tsne_coords[idx, 1], c=[colors[color_idx]],\n",
        "                             alpha=0.8, s=20)\n",
        "        handles.append(scatter)\n",
        "\n",
        "    # Add legend with cluster labels\n",
        "    ax.legend(handles, [label for _, label, _ in cluster_mapping], loc='best')\n",
        "\n",
        "    # Set plot title and axis labels\n",
        "    ax.set_title(name)\n",
        "    ax.set_xlabel(r't-SNE $p_1$')\n",
        "    ax.set_ylabel(r't-SNE $p_2$')\n",
        "\n",
        "    # Apply uniform axis limits if provided\n",
        "    if axis_limits:\n",
        "        ax.set_xlim(axis_limits[0])\n",
        "        ax.set_ylim(axis_limits[1])\n",
        "\n",
        "def assign_semantic_labels(phi_scores, n_clusters):\n",
        "    \"\"\"\n",
        "    Assign semantic labels to clusters based on Phi coefficient scores for 2-cluster analysis.\n",
        "\n",
        "    Args:\n",
        "        phi_scores (dict): Phi scores mapping clusters to semantic category scores.\n",
        "        n_clusters (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping of cluster IDs (e.g., 'Cluster 1') to semantic labels (Technical or Narrative).\n",
        "    \"\"\"\n",
        "    labels = {}\n",
        "    if n_clusters == 2:\n",
        "        # Calculate total scores for Technical (math + code) and Narrative (literary + dynamics)\n",
        "        tech_scores, narr_scores = [], []\n",
        "        for cluster in phi_scores:\n",
        "            scores = phi_scores[cluster]\n",
        "            tech_total = scores['math'] + scores['code']\n",
        "            narr_total = scores['literary'] + scores['dynamics']\n",
        "            tech_scores.append(tech_total)\n",
        "            narr_scores.append(narr_total)\n",
        "\n",
        "        # Assign labels based on which category has the higher total score\n",
        "        labels['Cluster 1'] = 'Technical' if tech_scores[0] > narr_scores[0] else 'Narrative'\n",
        "        labels['Cluster 2'] = 'Narrative' if tech_scores[0] > narr_scores[0] else 'Technical'\n",
        "    return labels\n",
        "\n",
        "def get_axis_limits(all_tsne_coords):\n",
        "    \"\"\"\n",
        "    Compute uniform axis limits for t-SNE plots with padding.\n",
        "\n",
        "    Args:\n",
        "        all_tsne_coords (list): List of t-SNE coordinates for all models.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple of (x_limits, y_limits) for uniform scaling.\n",
        "    \"\"\"\n",
        "    x_min, x_max = np.inf, -np.inf\n",
        "    y_min, y_max = np.inf, -np.inf\n",
        "    for coords in all_tsne_coords:\n",
        "        if coords.size > 0:\n",
        "            x_min = min(x_min, coords[:, 0].min())\n",
        "            x_max = max(x_max, coords[:, 0].max())\n",
        "            y_min = min(y_min, coords[:, 1].min())\n",
        "            y_max = max(y_max, coords[:, 1].max())\n",
        "    x_pad = 0.1 * (x_max - x_min)  # 10% padding on x-axis\n",
        "    y_pad = 0.1 * (y_max - y_min)  # 10% padding on y-axis\n",
        "    return (x_min - x_pad, x_max + x_pad), (y_min - y_pad, y_max + y_pad)\n",
        "\n",
        "# Main execution logic for clustering and visualization\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the list of models to compare\n",
        "    models = ['AstroSage', 'AstroLlama2', 'Claude 3.7 Sonnet', 'Grok 3 Mini (high)', 'Llama 3.1 8B', 'o4-mini']\n",
        "    n_clusters = 2  # Number of clusters for spectral clustering\n",
        "\n",
        "    # Set up a 2x3 subplot grid for the six models\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # First pass: Compute t-SNE coordinates to determine uniform axis limits\n",
        "    all_tsne_coords = []\n",
        "    for i, name in enumerate(models):\n",
        "        unique_bigrams = []\n",
        "        document_blocks = []\n",
        "\n",
        "        # Load and parse model output file\n",
        "        filename = f\"{name}.txt\"\n",
        "        try:\n",
        "            with open(filename, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                # Extract reply blocks using regex patterns\n",
        "                document_blocks = re.findall(r'reply\\s*:\\s*(.*?)(?=\\nuser_answer_str:|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    document_blocks = re.findall(r'The answer should either be numeric or symbolic \\(not words\\)\\s*.*?\\n(.*?)(?=\\nMESSAGE|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    print(f\"Warning: No reply blocks found in {filename}.\")\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "                    continue\n",
        "\n",
        "                # Tokenize and generate bigrams\n",
        "                text = \"\\n\".join(document_blocks)\n",
        "                tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "                if not tokens:\n",
        "                    print(f\"Warning: No tokens extracted from {filename}.\")\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "                    continue\n",
        "\n",
        "                bigrams = [' '.join(tokens[j:j+2]) for j in range(len(tokens)-1)]\n",
        "                unique_bigrams = list(Counter(bigrams).keys())\n",
        "                if not unique_bigrams:\n",
        "                    print(f\"Warning: No unique bigrams generated from {filename}.\")\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "                    continue\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: {filename} not found.\")\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "            continue\n",
        "\n",
        "        # Ensure enough bigrams for clustering\n",
        "        if len(unique_bigrams) < n_clusters:\n",
        "            print(f\"Warning: Not enough unique bigrams ({len(unique_bigrams)}) in {name} to form {n_clusters} clusters.\")\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, None)\n",
        "            continue\n",
        "\n",
        "        # Vectorize bigrams and apply spectral clustering\n",
        "        vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "        X = vectorizer.fit_transform(unique_bigrams).toarray()\n",
        "        labels = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans', random_state=42).fit_predict(X)\n",
        "\n",
        "        # Calculate Phi coefficients and assign semantic labels\n",
        "        phi_scores = calculate_phi_coefficient(unique_bigrams, labels, document_blocks, n_clusters)\n",
        "        semantic_labels = assign_semantic_labels(phi_scores, n_clusters)\n",
        "\n",
        "        # Compute t-SNE coordinates for axis limit calculation\n",
        "        perplexity = min(30, max(5, len(unique_bigrams)-1))\n",
        "        tsne_coords = TSNE(n_components=2, perplexity=perplexity, random_state=42).fit_transform(X)\n",
        "        all_tsne_coords.append(tsne_coords)\n",
        "\n",
        "        # Print semantic labels for verification\n",
        "        print(f\"\\n{name} ({n_clusters} clusters) Semantic Labels:\")\n",
        "        for cluster, meaning in semantic_labels.items():\n",
        "            print(f\"{cluster}: {meaning}\")\n",
        "\n",
        "    # Compute uniform axis limits across all models\n",
        "    axis_limits = get_axis_limits(all_tsne_coords) if all_tsne_coords else None\n",
        "\n",
        "    # Second pass: Plot the clusters with uniform axis limits\n",
        "    for i, name in enumerate(models):\n",
        "        unique_bigrams = []\n",
        "        document_blocks = []\n",
        "\n",
        "        filename = f\"{name}.txt\"\n",
        "        try:\n",
        "            with open(filename, 'r', encoding='utf-8') as file:\n",
        "                content = file.read()\n",
        "                document_blocks = re.findall(r'reply\\s*:\\s*(.*?)(?=\\nuser_answer_str:|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    document_blocks = re.findall(r'The answer should either be numeric or symbolic \\(not words\\)\\s*.*?\\n(.*?)(?=\\nMESSAGE|$)', content, re.DOTALL | re.IGNORECASE)\n",
        "                if not document_blocks:\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "                    continue\n",
        "                text = \"\\n\".join(document_blocks)\n",
        "                tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "                if not tokens:\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "                    continue\n",
        "                bigrams = [' '.join(tokens[j:j+2]) for j in range(len(tokens)-1)]\n",
        "                unique_bigrams = list(Counter(bigrams).keys())\n",
        "                if not unique_bigrams:\n",
        "                    plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "                    continue\n",
        "        except FileNotFoundError:\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "            continue\n",
        "\n",
        "        if len(unique_bigrams) < n_clusters:\n",
        "            plot_clusters([], [], n_clusters, name, axes[i], {}, axis_limits)\n",
        "            continue\n",
        "\n",
        "        # Vectorize, cluster, and label for plotting\n",
        "        vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "        X = vectorizer.fit_transform(unique_bigrams).toarray()\n",
        "        labels = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans', random_state=42).fit_predict(X)\n",
        "        phi_scores = calculate_phi_coefficient(unique_bigrams, labels, document_blocks, n_clusters)\n",
        "        semantic_labels = assign_semantic_labels(phi_scores, n_clusters)\n",
        "\n",
        "        # Plot the t-SNE clusters\n",
        "        plot_clusters(unique_bigrams, labels, n_clusters, name, axes[i], semantic_labels, axis_limits)\n",
        "\n",
        "    # Add a super title and adjust layout for the final plot\n",
        "    plt.suptitle(f\"t-SNE Clustering Comparison ({n_clusters} Clusters)\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "\n",
        "    # Save the plot as an SVG file for high-quality rendering in the paper\n",
        "    plt.savefig(f\"comparison_models_{n_clusters}_clusters.svg\", bbox_inches='tight', dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7UyIRLT7R_bZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference Clustering Analysis for PSA OpenBench Gamma Dataset\n",
        "# Date: May 15, 2025\n",
        "# Description: This script performs a semantic analysis of the PSA OpenBench Gamma dataset using bigram extraction,\n",
        "# spectral clustering, t-SNE visualization, and Phi coefficient-based semantic labeling for 2, 4, and 8 clusters.\n",
        "# The analysis serves as a reference for comparing language model outputs, as described in the accompanying paper.\n",
        "\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from collections import Counter\n",
        "from scipy.stats import chi2_contingency\n",
        "import warnings\n",
        "\n",
        "# Configure matplotlib to use serif font for better readability in plots\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "\n",
        "def calculate_phi_coefficient(bigrams, labels, document_blocks, n_clusters):\n",
        "    \"\"\"\n",
        "    Calculate Phi coefficients to quantify the association between bigrams and semantic categories.\n",
        "\n",
        "    Args:\n",
        "        bigrams (list): List of bigrams extracted from reference solutions.\n",
        "        labels (np.ndarray): Cluster labels assigned to each bigram.\n",
        "        document_blocks (list): List of solution text blocks from the PSA OpenBench Gamma dataset.\n",
        "        n_clusters (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Phi scores for each cluster, mapping clusters to category scores (e.g., math, code).\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store Phi scores for each cluster and semantic category\n",
        "    phi_scores = {i: Counter() for i in range(n_clusters)}\n",
        "    doc_types = ['math', 'code', 'literary', 'dynamics']\n",
        "\n",
        "    # Classify each document block into a semantic category using regex patterns\n",
        "    doc_classifications = []\n",
        "    for block in document_blocks:\n",
        "        block_lower = block.lower()\n",
        "        if re.search(r'[0-9]+(\\.[0-9]+)?|[μΔ√πθ]', block_lower):  # Math: numbers and symbols\n",
        "            doc_classifications.append('math')\n",
        "        elif re.search(r'(def|class|if|for|while|int|str|return|[a-z_][a-z0-9_]*(\\.[a-z0-9_]+)?)', block_lower):  # Code: keywords and identifiers\n",
        "            doc_classifications.append('code')\n",
        "        elif re.search(r'(process|change|interaction|system|flow|evolve|adapt|transition|state|time)', block_lower):  # Dynamics: processes and temporal terms\n",
        "            doc_classifications.append('dynamics')\n",
        "        else:  # Literary: narrative or descriptive content\n",
        "            doc_classifications.append('literary')\n",
        "\n",
        "    total_docs = len(document_blocks)\n",
        "    doc_type_counts = Counter(doc_classifications)\n",
        "\n",
        "    # Compute Phi coefficient for each bigram and semantic category\n",
        "    for i, bigram in enumerate(bigrams):\n",
        "        cluster = labels[i]\n",
        "        bigram_lower = bigram.lower()\n",
        "\n",
        "        for doc_type in doc_types:\n",
        "            if doc_type_counts[doc_type] == 0:  # Skip if no documents of this type exist\n",
        "                phi_scores[cluster][doc_type] = 0\n",
        "                continue\n",
        "\n",
        "            # Construct a 2x2 contingency table for the bigram and document type\n",
        "            a = sum(1 for block, dtype in zip(document_blocks, doc_classifications)\n",
        "                    if dtype == doc_type and bigram_lower in block.lower())  # Bigram present, doc is of type\n",
        "            b = doc_type_counts[doc_type] - a  # Bigram absent, doc is of type\n",
        "            c = sum(1 for block, dtype in zip(document_blocks, doc_classifications)\n",
        "                    if dtype != doc_type and bigram_lower in block.lower())  # Bigram present, doc not of type\n",
        "            d = (total_docs - doc_type_counts[doc_type]) - c  # Bigram absent, doc not of type\n",
        "\n",
        "            # Apply Laplace smoothing (alpha=0.5) to avoid division by zero\n",
        "            smoothing = 0.5\n",
        "            table = np.array([[a + smoothing, b + smoothing],\n",
        "                              [c + smoothing, d + smoothing]])\n",
        "\n",
        "            # Compute chi-squared statistic and Phi coefficient\n",
        "            chi2, _, _, _ = chi2_contingency(table, correction=False)\n",
        "            phi = np.sqrt(chi2 / (total_docs + 4 * smoothing)) if total_docs > 0 else 0\n",
        "            phi_scores[cluster][doc_type] += phi\n",
        "\n",
        "    return phi_scores\n",
        "\n",
        "def plot_clusters(bigrams, labels, n_clusters, name, ax, semantic_labels, axis_limits):\n",
        "    \"\"\"\n",
        "    Plot t-SNE clusters for reference solutions with semantic labels.\n",
        "\n",
        "    Args:\n",
        "        bigrams (list): List of bigrams to plot.\n",
        "        labels (np.ndarray): Cluster labels for the bigrams.\n",
        "        n_clusters (int): Number of clusters.\n",
        "        name (str): Model name for the plot title (set to 'Reference' here).\n",
        "        ax (matplotlib.axes.Axes): Axis object to plot on.\n",
        "        semantic_labels (dict): Mapping of cluster IDs to semantic labels.\n",
        "        axis_limits (tuple): Tuple of (x_limits, y_limits) for uniform axis scaling.\n",
        "    \"\"\"\n",
        "    # Handle empty or insufficient data\n",
        "    if not bigrams:\n",
        "        ax.text(0.5, 0.5, 'Data Unavailable', ha='center', va='center', fontsize=12, color='red')\n",
        "        ax.set_title(name)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        return\n",
        "\n",
        "    # Vectorize bigrams into a sparse matrix of token counts\n",
        "    vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "    X = vectorizer.fit_transform(bigrams).toarray()\n",
        "\n",
        "    # Apply t-SNE for 2D visualization, dynamically tuning perplexity\n",
        "    perplexity = min(30, max(5, len(bigrams)-1))\n",
        "    tsne_coords = TSNE(n_components=2, perplexity=perplexity, random_state=42).fit_transform(X)\n",
        "\n",
        "    # Define label order and colors based on number of clusters\n",
        "    if n_clusters == 2:\n",
        "        label_order = ['Technical', 'Narrative']\n",
        "        colors = ['#1f77b4', '#ff7f0e']\n",
        "    elif n_clusters == 4:\n",
        "        label_order = ['Math', 'Code', 'Literary', 'Dynamics']\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
        "    else:  # n_clusters == 8\n",
        "        label_order = ['Math Basics', 'Math Advanced', 'Code Syntax', 'Code Structure',\n",
        "                       'Literary Prose', 'Literary Themes', 'Dynamics Basics', 'Dynamics Advanced']\n",
        "        colors = ['#1f77b4', '#4a90e2', '#ff7f0e', '#ffaa5e', '#2ca02c', '#6abe6a', '#d62728', '#ff6666']\n",
        "\n",
        "    # Map clusters to labels and sort by predefined order\n",
        "    cluster_mapping = []\n",
        "    for i in range(n_clusters):\n",
        "        cluster_label = semantic_labels.get(f'Cluster {i+1}', f'Cluster {i+1}')\n",
        "        if cluster_label in label_order:\n",
        "            cluster_mapping.append((i, cluster_label, label_order.index(cluster_label)))\n",
        "\n",
        "    cluster_mapping.sort(key=lambda x: x[2])\n",
        "\n",
        "    # Clear any existing legend\n",
        "    if ax.get_legend():\n",
        "        ax.get_legend().remove()\n",
        "\n",
        "    # Plot and collect handles for legend\n",
        "    handles = []\n",
        "    for i, cluster_label, color_idx in cluster_mapping:\n",
        "        idx = np.where(labels == i)[0]\n",
        "        scatter = ax.scatter(tsne_coords[idx, 0], tsne_coords[idx, 1], c=[colors[color_idx]],\n",
        "                             alpha=0.8, s=20)\n",
        "        handles.append(scatter)\n",
        "\n",
        "    # Set legend with explicit handles and labels\n",
        "    ax.legend(handles, [label for _, label, _ in cluster_mapping], loc='best')\n",
        "\n",
        "    ax.set_title(f'Reference ({n_clusters} clusters)')\n",
        "    ax.set_xlabel(r't-SNE $p_1$')\n",
        "    ax.set_ylabel(r't-SNE $p_2$')\n",
        "\n",
        "    # Apply shared axis limits\n",
        "    if axis_limits:\n",
        "        ax.set_xlim(axis_limits[0])\n",
        "        ax.set_ylim(axis_limits[1])\n",
        "\n",
        "def assign_semantic_labels(phi_scores, n_clusters):\n",
        "    \"\"\"\n",
        "    Assign semantic labels to clusters based on Phi coefficient scores.\n",
        "\n",
        "    Args:\n",
        "        phi_scores (dict): Phi scores mapping clusters to semantic category scores.\n",
        "        n_clusters (int): Number of clusters.\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping of cluster IDs (e.g., 'Cluster 1') to semantic labels.\n",
        "    \"\"\"\n",
        "    labels = {}\n",
        "    if n_clusters == 2:\n",
        "        # Calculate total scores for Technical (math + code) and Narrative (literary + dynamics)\n",
        "        tech_scores, narr_scores = [], []\n",
        "        for cluster in phi_scores:\n",
        "            scores = phi_scores[cluster]\n",
        "            tech_total = scores['math'] + scores['code']\n",
        "            narr_total = scores['literary'] + scores['dynamics']\n",
        "            tech_scores.append(tech_total)\n",
        "            narr_scores.append(narr_total)\n",
        "\n",
        "        # Assign labels based on which category has the higher total score\n",
        "        labels['Cluster 1'] = 'Technical' if tech_scores[0] > narr_scores[0] else 'Narrative'\n",
        "        labels['Cluster 2'] = 'Narrative' if tech_scores[0] > narr_scores[0] else 'Technical'\n",
        "    elif n_clusters == 4:\n",
        "        # Determine the dominant semantic category for each cluster\n",
        "        sorted_scores = {k: max(v.items(), key=lambda x: x[1])[0] for k, v in phi_scores.items()}\n",
        "        assigned_types = ['Math', 'Code', 'Literary', 'Dynamics']\n",
        "        used_types = set()\n",
        "        for cluster in range(n_clusters):\n",
        "            doc_type = sorted_scores[cluster]\n",
        "            if doc_type == 'math' and 'Math' not in used_types:\n",
        "                labels[f'Cluster {cluster+1}'] = 'Math'\n",
        "                used_types.add('Math')\n",
        "            elif doc_type == 'code' and 'Code' not in used_types:\n",
        "                labels[f'Cluster {cluster+1}'] = 'Code'\n",
        "                used_types.add('Code')\n",
        "            elif doc_type == 'literary' and 'Literary' not in used_types:\n",
        "                labels[f'Cluster {cluster+1}'] = 'Literary'\n",
        "                used_types.add('Literary')\n",
        "            elif doc_type == 'dynamics' and 'Dynamics' not in used_types:\n",
        "                labels[f'Cluster {cluster+1}'] = 'Dynamics'\n",
        "                used_types.add('Dynamics')\n",
        "            else:\n",
        "                # If a category is already used, assign the next available label\n",
        "                for t in assigned_types:\n",
        "                    if t not in used_types:\n",
        "                        labels[f'Cluster {cluster+1}'] = t\n",
        "                        used_types.add(t)\n",
        "                        break\n",
        "    elif n_clusters == 8:\n",
        "        # Collect scores for each semantic category across clusters\n",
        "        math_scores = []\n",
        "        code_scores = []\n",
        "        literary_scores = []\n",
        "        dynamics_scores = []\n",
        "\n",
        "        for cluster in range(n_clusters):\n",
        "            scores = phi_scores[cluster]\n",
        "            math_scores.append(scores['math'])\n",
        "            code_scores.append(scores['code'])\n",
        "            literary_scores.append(scores['literary'])\n",
        "            dynamics_scores.append(scores['dynamics'])\n",
        "\n",
        "        # Sort clusters by their association with each category\n",
        "        math_pairs = sorted([(score, idx) for idx, score in enumerate(math_scores)], reverse=True)\n",
        "        code_pairs = sorted([(score, idx) for idx, score in enumerate(code_scores)], reverse=True)\n",
        "        literary_pairs = sorted([(score, idx) for idx, score in enumerate(literary_scores)], reverse=True)\n",
        "        dynamics_pairs = sorted([(score, idx) for idx, score in enumerate(dynamics_scores)], reverse=True)\n",
        "\n",
        "        # Assign labels to clusters, ensuring no cluster is assigned multiple labels\n",
        "        used_clusters = set()\n",
        "        assigned_labels = [\n",
        "            ('Math Basics', math_pairs[0][1]), ('Math Advanced', math_pairs[1][1]),\n",
        "            ('Code Syntax', code_pairs[0][1]), ('Code Structure', code_pairs[1][1]),\n",
        "            ('Literary Prose', literary_pairs[0][1]), ('Literary Themes', literary_pairs[1][1]),\n",
        "            ('Dynamics Basics', dynamics_pairs[0][1]), ('Dynamics Advanced', dynamics_pairs[1][1])\n",
        "        ]\n",
        "\n",
        "        for label, cluster_idx in assigned_labels:\n",
        "            if cluster_idx not in used_clusters:\n",
        "                labels[f'Cluster {cluster_idx+1}'] = label\n",
        "                used_clusters.add(cluster_idx)\n",
        "            else:\n",
        "                # If a cluster is already used, find the next highest-scoring unused cluster\n",
        "                for _, next_cluster in math_pairs + code_pairs + literary_pairs + dynamics_pairs:\n",
        "                    if next_cluster not in used_clusters:\n",
        "                        labels[f'Cluster {next_cluster+1}'] = label\n",
        "                        used_clusters.add(next_cluster)\n",
        "                        break\n",
        "\n",
        "    return labels\n",
        "\n",
        "def get_axis_limits(all_tsne_coords):\n",
        "    \"\"\"\n",
        "    Compute uniform axis limits for t-SNE plots with padding.\n",
        "\n",
        "    Args:\n",
        "        all_tsne_coords (list): List of t-SNE coordinates for all cluster configurations.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple of (x_limits, y_limits) for uniform scaling.\n",
        "    \"\"\"\n",
        "    x_min, x_max = np.inf, -np.inf\n",
        "    y_min, y_max = np.inf, -np.inf\n",
        "    for coords in all_tsne_coords:\n",
        "        if coords.size > 0:\n",
        "            x_min = min(x_min, coords[:, 0].min())\n",
        "            x_max = max(x_max, coords[:, 0].max())\n",
        "            y_min = min(y_min, coords[:, 1].min())\n",
        "            y_max = max(y_max, coords[:, 1].max())\n",
        "    x_pad = 0.1 * (x_max - x_min)  # 10% padding on x-axis\n",
        "    y_pad = 0.1 * (y_max - y_min)  # 10% padding on y-axis\n",
        "    return (x_min - x_pad, x_max + x_pad), (y_min - y_pad, y_max + y_pad)\n",
        "\n",
        "# Main execution logic for reference clustering analysis\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the number of clusters to analyze\n",
        "    n_clusters_list = [2, 4, 8]\n",
        "\n",
        "    # Set up a 1x3 subplot grid for the three cluster configurations\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # First pass: Compute t-SNE coordinates to determine uniform axis limits\n",
        "    all_tsne_coords = []\n",
        "    for i, n_clusters in enumerate(n_clusters_list):\n",
        "        unique_bigrams = []\n",
        "        document_blocks = []\n",
        "\n",
        "        # Load and parse the PSA OpenBench Gamma dataset from JSON file\n",
        "        try:\n",
        "            with open('PSA_OpenBench_Gamma.json', 'r', encoding='utf-8') as f:\n",
        "                dataset = json.load(f)\n",
        "            # Extract solution blocks from the nested JSON structure\n",
        "            document_blocks = [question['Solution'] for entry in dataset for question in entry['Questions']]\n",
        "            if not document_blocks:\n",
        "                print(f\"Warning: No solutions found in PSA_OpenBench_Gamma.json.\")\n",
        "                plot_clusters([], [], n_clusters, 'Reference', axes[i], {}, None)\n",
        "                continue\n",
        "\n",
        "        except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:\n",
        "            print(f\"Warning: Error processing PSA_OpenBench_Gamma.json: {e}\")\n",
        "            plot_clusters([], [], n_clusters, 'Reference', axes[i], {}, None)\n",
        "            continue\n",
        "\n",
        "        # Tokenize and generate bigrams\n",
        "        text = \"\\n\".join(document_blocks)\n",
        "        tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "        if not tokens:\n",
        "            print(f\"Warning: No tokens extracted from reference solutions.\")\n",
        "            plot_clusters([], [], n_clusters, 'Reference', axes[i], {}, None)\n",
        "            continue\n",
        "\n",
        "        bigrams = [' '.join(tokens[j:j+2]) for j in range(len(tokens)-1)]\n",
        "        unique_bigrams = list(Counter(bigrams).keys())\n",
        "        if not unique_bigrams:\n",
        "            print(f\"Warning: No unique bigrams generated from reference solutions.\")\n",
        "            plot_clusters([], [], n_clusters, 'Reference', axes[i], {}, None)\n",
        "            continue\n",
        "\n",
        "        # Ensure enough bigrams for clustering\n",
        "        if len(unique_bigrams) < n_clusters:\n",
        "            print(f\"Warning: Not enough unique bigrams ({len(unique_bigrams)}) in Reference to form {n_clusters} clusters.\")\n",
        "            plot_clusters([], [], n_clusters, 'Reference', axes[i], {}, None)\n",
        "            continue\n",
        "\n",
        "        # Vectorize bigrams and apply spectral clustering\n",
        "        vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "        X = vectorizer.fit_transform(unique_bigrams).toarray()\n",
        "        labels = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans', random_state=42).fit_predict(X)\n",
        "\n",
        "        # Calculate Phi coefficients and assign semantic labels\n",
        "        phi_scores = calculate_phi_coefficient(unique_bigrams, labels, document_blocks, n_clusters)\n",
        "        semantic_labels = assign_semantic_labels(phi_scores, n_clusters)\n",
        "\n",
        "        # Compute t-SNE coordinates for axis limit calculation\n",
        "        perplexity = min(30, max(5, len(unique_bigrams)-1))\n",
        "        tsne_coords = TSNE(n_components=2, perplexity=perplexity, random_state=42).fit_transform(X)\n",
        "        all_tsne_coords.append(tsne_coords)\n",
        "\n",
        "        # Print semantic labels for verification\n",
        "        print(f\"\\nReference ({n_clusters} clusters) Semantic Labels:\")\n",
        "        for cluster, meaning in semantic_labels.items():\n",
        "            print(f\"{cluster}: {meaning}\")\n",
        "\n",
        "        # Initial plot without shared axis limits\n",
        "        plot_clusters(unique_bigrams, labels, n_clusters, 'Reference', axes[i], semantic_labels, None)\n",
        "\n",
        "    # Compute uniform axis limits across all cluster configurations\n",
        "    axis_limits = get_axis_limits(all_tsne_coords) if all_tsne_coords else None\n",
        "\n",
        "    # Second pass: Plot with uniform axis limits\n",
        "    for i, n_clusters in enumerate(n_clusters_list):\n",
        "        unique_bigrams = []\n",
        "        document_blocks = []\n",
        "\n",
        "        # Reload and parse the dataset\n",
        "        try:\n",
        "            with open('PSA_OpenBench_Gamma.json', 'r', encoding='utf-8') as f:\n",
        "                dataset = json.load(f)\n",
        "            document_blocks = [question['Solution'] for entry in dataset for question in entry['Questions']]\n",
        "            if not document_blocks:\n",
        "                continue\n",
        "            text = \"\\n\".join(document_blocks)\n",
        "            tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "            if not tokens:\n",
        "                continue\n",
        "            bigrams = [' '.join(tokens[j:j+2]) for j in range(len(tokens)-1)]\n",
        "            unique_bigrams = list(Counter(bigrams).keys())\n",
        "            if not unique_bigrams:\n",
        "                continue\n",
        "\n",
        "        except (FileNotFoundError, json.JSONDecodeError, KeyError):\n",
        "            continue\n",
        "\n",
        "        if len(unique_bigrams) < n_clusters:\n",
        "            continue\n",
        "\n",
        "        # Vectorize, cluster, and label for plotting\n",
        "        vectorizer = CountVectorizer(max_features=2000, token_pattern=r'\\b\\w+\\b', lowercase=True)\n",
        "        X = vectorizer.fit_transform(unique_bigrams).toarray()\n",
        "        labels = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans', random_state=42).fit_predict(X)\n",
        "        phi_scores = calculate_phi_coefficient(unique_bigrams, labels, document_blocks, n_clusters)\n",
        "        semantic_labels = assign_semantic_labels(phi_scores, n_clusters)\n",
        "\n",
        "        # Plot with shared axis limits\n",
        "        plot_clusters(unique_bigrams, labels, n_clusters, 'Reference', axes[i], semantic_labels, axis_limits)\n",
        "\n",
        "    # Add a super title and adjust layout for the final plot\n",
        "    plt.suptitle('Reference t-SNE Clustering (2, 4, 8 Clusters)', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save the plot as an SVG file for high-quality rendering in the paper\n",
        "    plt.savefig('reference_clusters_2_4_8.svg', bbox_inches='tight', dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MHB-iDvnTRXD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}