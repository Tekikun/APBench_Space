{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import openai\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "from utils import extract_numbers,find_first_empty_index,process_scientific_notation,extract_numeric_answer\n",
    "openai.api_key  = \"YOUR OPENAI API KEY\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_sentence = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download APBench_Gamma.json from woodywu/APBench \n",
    "filename = 'APBench_Gamma.json'\n",
    "data = json.loads(Path(filename).read_text())\n",
    "# Print the tenth problem and the 1st question among the questions\n",
    "index, order = 10, 1\n",
    "content   = data[index]['Content']\n",
    "question_solution_so_on = data[index]['Questions'][order]\n",
    "print(\"Question:\", question_solution_so_on['Question'])\n",
    "print(\"Solution:\", question_solution_so_on['Solution'])\n",
    "print(\"Answer:\", question_solution_so_on['Answer'])\n",
    "print(\"Format:\", question_solution_so_on['Format'])\n",
    "print(\"Source:\", question_solution_so_on['Source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    return model_sentence.encode(text)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = '''The Viking Mars orbiter probe was placed in a circular orbit 17,000 km above the Martian surface. Doppler measurements of the transmitted signals from the probe indicated that it was at an orbital velocity, \\( V_V \\), of 1.46 km/s. The diameter of Mars, \\( d_M \\), is 6,770 km and the radius of the Mars orbit about the Sun, \\( a_M \\), is 1.524 AU. Note: 1 AU = 1.495 × 10^8 km; 1 year = 3.156 × 10^7 s. Calculate the mass of Mars, \\( M_M \\), in terms of the mass of the Sun, \\( M_S \\). Without any extra words, please specify the answer in the final sentence with this form: 'the answer is (answer)'. The answer should either be numeric or symbolic (not words).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"AstroMLab/AstroSage-8b\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AstroMLab/AstroSage-8b\")\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        # max_new_tokens=256,\n",
    "        # max_length=2024,\n",
    "        max_length=6000,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    response = outputs[0][inputs['input_ids'].shape[-1]:]\n",
    "    decoded = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    return decoded\n",
    "\n",
    "# Example usage\n",
    "prompt = \"\"\"\n",
    "You are an expert in general astrophysics. Your task is to answer the following question:\n",
    "What are the main components of a galaxy?\n",
    "\"\"\"\n",
    "response = generate_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in range(1):\n",
    "    # messages = [\n",
    "    #     {\"role\": \"user\", \"content\": msg}\n",
    "    # ]\n",
    "    # generated_text = pipe(\n",
    "    #     messages,\n",
    "    #     max_length = 2024\n",
    "    # )\n",
    "    reply = generate_response(msg)\n",
    "    print(reply)\n",
    "print(f'Elapsed time: {(time.time()-start_time)/1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[10]['Questions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_result = []  # Universal array of results\n",
    "\n",
    "for idx in range(len(data)): #\n",
    "    # idx = 2\n",
    "    print(\"_____________\")\n",
    "    print(\"ID\", idx)\n",
    "    print(\"current_match_result: \", match_result)\n",
    "\n",
    "\n",
    "    content_str = data[idx]['Content']\n",
    "    questions_solutions = data[idx]['Questions']\n",
    "    question_strs = [entry['Question'] for entry in questions_solutions]\n",
    "    solution_strs = [entry['Solution'] for entry in questions_solutions]\n",
    "    answer_strs = [entry['Answer'] for entry in questions_solutions]\n",
    "    format_strs = [entry['Format'] for entry in questions_solutions]\n",
    "    match_results_local = [[] for entry in questions_solutions]\n",
    "\n",
    "    # print(\"content: \",content_str)\n",
    "\n",
    "    correct_QAs = []\n",
    "    for q_idx, question_str in enumerate(question_strs):\n",
    "        print(\"______\")\n",
    "        print(\"q_idx: \", q_idx)\n",
    "        format_str = format_strs[q_idx]\n",
    "        solution_str = solution_strs[q_idx]\n",
    "        answer_str = answer_strs[q_idx]\n",
    "        print(\"Answer: \", answer_str)\n",
    "        all_correct_QAs = \" \".join(correct_QAs)\n",
    "        msg = f\"{all_correct_QAs} {content_str} {question_str} Without any extra words, please specify the answer in the final sentence with this form: 'the answer is (answer)'. The answer should either be numeric or symbolic (not words).\"\n",
    "\n",
    "        print(\"msg\", msg)\n",
    "        \n",
    "        # calling model\n",
    "        reply = generate_response(msg)\n",
    "        print(\"reply: \", reply)\n",
    "        \n",
    "        pattern = r'the answer is \\s*(.*)|The answer is \\s*(.*)' \n",
    "        match = re.search(pattern, reply)\n",
    "        try:\n",
    "            user_answer_str = match.group(0)\n",
    "            print(\"user_answer_str: \", user_answer_str)\n",
    "        except:\n",
    "            user_answer_str = None\n",
    "\n",
    "        if 'message' == format_str:\n",
    "            print(\"MESSAGE\")\n",
    "\n",
    "            score = 0\n",
    "\n",
    "            # embedding score\n",
    "            score_embedding = cosine_similarity(get_embedding(reply), get_embedding(answer_str))\n",
    "            print(f\"similarity: {score_embedding}\")\n",
    "            \n",
    "            # gpt score\n",
    "            # gpt prompt\n",
    "            message = f\"\"\"\n",
    "            Compare the following reply with the expected answer and evaluate their alignment using these criteria:\n",
    "\n",
    "            1. **Relevance:** Does the reply address the scientific goals and concepts mentioned in the expected answer?  \n",
    "            2. **Completeness:** Does the reply cover the key points about the need for a deep periapsis and a large range of distances?  \n",
    "            3. **Accuracy:** Is the reasoning in the reply correct and consistent with the scientific explanation?  \n",
    "\n",
    "            Question: {question_str}  \n",
    "            Expected Answer: {answer_str}  \n",
    "            Reply: {reply}  \n",
    "\n",
    "            After evaluating, provide a similarity score between 0 and 10, where:  \n",
    "            - **10** means the reply perfectly aligns with the expected answer.  \n",
    "            - **0** means the reply does not align at all.  \n",
    "\n",
    "            Only return the numeric score as your final output.\n",
    "            \"\"\"\n",
    "\n",
    "            # response part\n",
    "            response = openai.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                temperature=0,\n",
    "            )\n",
    "            gpt_similarity = response.choices[0].message.content\n",
    "            score_gpt = float(gpt_similarity) / 10 \n",
    "\n",
    "            # combining scores\n",
    "            score = score_embedding*0.5 + score_gpt*0.5\n",
    "            print(f\"score_gpt: {score_gpt}\")\n",
    "            print(f\"SCORE: {score}\")\n",
    "            \n",
    "            if score > 0.6: # more than 60% affirmative\n",
    "                match_results_local[q_idx] = 1\n",
    "                extra_str = \"For reference, a previous question: '\" + content_str + question_str + \" '  has answer \" + reply\n",
    "                correct_QAs.append(extra_str)\n",
    "            else:\n",
    "                match_results_local[q_idx] = 0 \n",
    "\n",
    "            ## formated output\n",
    "            print(f'LLM answer: {reply}')\n",
    "            print(f'Reference answer: {answer_str}')\n",
    "            print('True' if match_results_local[q_idx]==1 else 'False')\n",
    "           \n",
    "        if 'numeric' == format_str:\n",
    "            print(\"NUMERIC\")\n",
    "\n",
    "            power = []\n",
    "            try:\n",
    "                if user_answer_str is None:\n",
    "                    user_answer_str = ''\n",
    "                    raise TypeError(\"user_answer_str cannot be None\")\n",
    "                user_answer_num_str = extract_numeric_answer(user_answer_str)\n",
    "                user_answer_num = extract_numbers(user_answer_num_str)[0]\n",
    "            except:\n",
    "                # check if answer string has power expression\n",
    "                user_answer_str, power = process_scientific_notation(user_answer_str)\n",
    "                user_answer_num = None\n",
    "\n",
    "            # when openai is able to handle the operation\n",
    "            if not power:\n",
    "                user_answer_num = user_answer_num\n",
    "                answer_str_num = extract_numbers(answer_str)[0]\n",
    "                match_results_local[q_idx] = 0\n",
    "            # other cases\n",
    "            else:\n",
    "                try:\n",
    "                    if power:\n",
    "                        print(\"pow\")\n",
    "                        user_answer_pow = extract_numbers(user_answer_str.strip())[2]\n",
    "                        user_answer_num = extract_numbers(user_answer_str.strip())[0] * 10**user_answer_pow\n",
    "                        power = extract_numbers(answer_str)[1]\n",
    "                        answer_str_num = extract_numbers(answer_str)[0] * 10**power\n",
    "\n",
    "                        print(\"user_answer_num\", user_answer_num)\n",
    "                        print(\"answer_str_num\", answer_str_num)\n",
    "\n",
    "                    else:\n",
    "                        print(\"no pow\")\n",
    "                        print(user_answer_str.strip())\n",
    "                        user_answer_num = extract_numbers(user_answer_str.strip())[0]\n",
    "                        answer_str_num = extract_numbers(answer_str)[0]\n",
    "\n",
    "                        print(\"user_answer_num\", user_answer_num)\n",
    "                        print(\"answer_str_num\", answer_str_num)\n",
    "\n",
    "                except:\n",
    "                    user_answer_num = None\n",
    "                    answer_str_num = None\n",
    "                    match_results_local[q_idx] = 0\n",
    "\n",
    "            if user_answer_num is not None and user_answer_num != 0:\n",
    "                \n",
    "                margin = 0.1 + 0.01 * math.log(abs(user_answer_num))\n",
    "                print(\"margin: \", margin)\n",
    "                if answer_str_num == user_answer_num:\n",
    "                    match_results_local[q_idx] = 1\n",
    "                    extra_str = \"For reference, a previous question: \" + question_str + \". It has answer \" + str(user_answer_num)\n",
    "                    correct_QAs.append(extra_str)\n",
    "                    # print(\"Exact Match\")\n",
    "                elif abs(answer_str_num - user_answer_num) / abs(user_answer_num) < abs(margin):\n",
    "                    match_results_local[q_idx] = 1\n",
    "                    extra_str = \"For reference, a previous question: \" + question_str + \". It has answer \" + str(user_answer_num)\n",
    "                    correct_QAs.append(extra_str)\n",
    "                    # print(\"Rough Match\")\n",
    "                else:\n",
    "                    match_results_local[q_idx] = 0\n",
    "                    # print(\"No Match\")\n",
    "            \n",
    "            ## formated output\n",
    "            print(f'LLM answer: {user_answer_str}')\n",
    "            print(f'Reference answer: {answer_str}')\n",
    "            print('True' if match_results_local[q_idx]==1 else 'False')\n",
    "\n",
    "    if match_results_local:\n",
    "        match_results_local_new = [item for item in match_results_local if not (isinstance(item, list) and len(item) == 0)]\n",
    "        print(\"local\", match_results_local_new)\n",
    "        match_result.extend(match_results_local_new)\n",
    "\n",
    "# Print the universal match result array\n",
    "print(\"Universal match result array:\", match_result)\n",
    "\n",
    "\n",
    "total_accuracy = sum(match_result) / len(match_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
